# TERVYX Amazon Claims ‚Äî Implementation Summary

## üéØ Mission Accomplished

All core pipeline components have been implemented with **Option B (Google Custom Search API)** approach as planned.

---

## ‚úÖ What's Been Implemented

### 1. **Product Discovery (scripts/1_generate_urls.py)** ‚úì
**Implementation**: Google Custom Search API integration

**Features**:
- ‚úÖ R cohort: Stratified sampling using Google CSE
- ‚úÖ T cohort: Keyword-based targeted discovery
- ‚úÖ ASIN extraction from Amazon URLs
- ‚úÖ Fallback to placeholder mode if API keys not set
- ‚úÖ Duplicate detection

**Usage**:
```bash
export GOOGLE_SEARCH_API_KEY="..."
export GOOGLE_SEARCH_CX="..."

python scripts/1_generate_urls.py \
  --plan configs/sampling_plan.yaml \
  --out data/raw/product_urls.csv \
  --sample 50
```

**Output**: CSV with ASIN, URL, cohort, method, stratum

---

### 2. **Page Scraping (scripts/2_scrape_pages.py)** ‚úì
**Implementation**: Ethical web scraping with full compliance

**Features**:
- ‚úÖ robots.txt compliance (urllib.robotparser)
- ‚úÖ Throttling (3s default, configurable)
- ‚úÖ Exponential backoff retries
- ‚úÖ Wayback Machine archival (via API)
- ‚úÖ HTML storage for reproducibility
- ‚úÖ Amazon-specific image extraction (product gallery + A+ content)
- ‚úÖ User-Agent transparency

**Usage**:
```bash
python scripts/2_scrape_pages.py \
  --in data/raw/product_urls.csv \
  --out data/extracted/pages.sha256.csv \
  --assets data/processed/assets_index.csv \
  --html-dir data/raw/html/ \
  --policy configs/scraping_policy.yaml
```

**Output**:
- `pages.sha256.csv`: Page metadata (ASIN, SHA256, Wayback URL, timestamp)
- `assets_index.csv`: Image URLs (ASIN, asset_id, URL, SHA256)
- `html/*.html`: Saved HTML files for reproducibility

---

### 3. **Claim Extraction (scripts/3_extract_claims.py)** ‚úì
**Implementation**: Gemini API (temp=0) + OCR fallback

**Features**:
- ‚úÖ Gemini 2.0 Flash integration (temperature=0, deterministic)
- ‚úÖ Amazon-specific HTML parsing (title, bullets, description, A+ content)
- ‚úÖ Structured JSON extraction (claim_type, implied_outcome, quantifiers)
- ‚úÖ Tesseract OCR for image-based claims
- ‚úÖ OCR confidence filtering (threshold=0.7)
- ‚úÖ Rule-based fallback (keyword matching)
- ‚úÖ LLM-assisted OCR cleanup (optional)

**Usage**:
```bash
export GEMINI_API_KEY="..."

python scripts/3_extract_claims.py \
  --pages data/extracted/pages.sha256.csv \
  --assets data/processed/assets_index.csv \
  --html-dir data/raw/html/ \
  --out data/extracted/claims_raw.jsonl \
  --policy configs/extraction_policy.yaml
```

**Output**: JSONL with extraction metadata + claims array

**Critical validation**: `if policy["temperature"] != 0: raise ValueError()`

---

### 4. **OCR Pipeline** ‚úì
**Implementation**: Tesseract + optional Gemini cleanup

**Features**:
- ‚úÖ pytesseract integration
- ‚úÖ Bounding box extraction
- ‚úÖ Confidence-based filtering
- ‚úÖ Claim keyword detection
- ‚úÖ LLM cleanup for OCR errors (optional, temp=0)

**Image Sources**:
- Main product image (`#landingImage`)
- Gallery images (`.a-dynamic-image`)
- A+ content images (`[data-a-dynamic-image]`)

---

### 5. **Sampling Weights (scripts/6_sampling_weights.py)** ‚úì
**Implementation**: Stratified weighting based on allocation targets

**Features**:
- ‚úÖ Weight = (target_proportion / sample_proportion)
- ‚úÖ Uses sampling_plan.yaml allocation as proxy for population
- ‚úÖ Handles missing strata gracefully
- ‚úÖ Detailed stratum distribution report
- ‚úÖ In-place CSV update

**Usage**:
```bash
python scripts/6_sampling_weights.py \
  --product data/processed/product_info.csv \
  --plan configs/sampling_plan.yaml
```

**Output**: Updates `product_info.csv` with `sampling_weight` column

---

## üì¶ New Files Created

### Documentation
- `SETUP.md`: Step-by-step setup guide (API keys, Tesseract, cost estimates)
- `.env.example`: Environment variable template
- `IMPLEMENTATION_SUMMARY.md`: This document

### Dependencies
- `requirements.txt`: Updated with:
  - `google-generativeai>=0.3.0` (Gemini SDK)
  - `google-api-python-client>=2.100.0` (Custom Search API)
  - `pytesseract>=0.3.10` (OCR)

### Pipeline
- Updated `Makefile` with `--html-dir` parameter

---

## üîß Configuration Files

All configs remain unchanged from original design:

- `configs/sampling_plan.yaml`: R/T cohort definitions (6 strata + 8 targeted nodes)
- `configs/scraping_policy.yaml`: Ethics-first settings (throttle=3s, robots.txt=true)
- `configs/extraction_policy.yaml`: LLM settings (model=gemini-2.0-flash-exp, temp=0)
- `configs/policy_hints.yaml`: Œ¶/K/L gate patterns

---

## üöÄ Running the Pipeline

### Quick Test (10 samples, no APIs required)
```bash
make test
```

This uses placeholder mode (no actual web requests or API calls).

### 50-Product Prototype (requires APIs)
```bash
# 1. Set environment variables
export GOOGLE_SEARCH_API_KEY="..."
export GOOGLE_SEARCH_CX="..."
export GEMINI_API_KEY="..."

# 2. Run pipeline
python scripts/1_generate_urls.py --plan configs/sampling_plan.yaml --out data/raw/product_urls.csv --sample 50
python scripts/2_scrape_pages.py --in data/raw/product_urls.csv --out data/extracted/pages.sha256.csv --assets data/processed/assets_index.csv --html-dir data/raw/html/ --policy configs/scraping_policy.yaml
python scripts/3_extract_claims.py --pages data/extracted/pages.sha256.csv --assets data/processed/assets_index.csv --html-dir data/raw/html/ --out data/extracted/claims_raw.jsonl --policy configs/extraction_policy.yaml
python scripts/4_normalize_to_csv.py --raw data/extracted/claims_raw.jsonl --product-urls data/raw/product_urls.csv --product-out data/processed/product_info.csv --claims-out data/processed/claims.csv --assets-in data/processed/assets_index.csv --hints configs/policy_hints.yaml
python scripts/5_qc_validate.py --schemas schemas/ --product data/processed/product_info.csv --claims data/processed/claims.csv --assets data/processed/assets_index.csv
python scripts/6_sampling_weights.py --product data/processed/product_info.csv --plan configs/sampling_plan.yaml
python scripts/7_pattern_report.py --claims data/processed/claims.csv --out data/reports/top_patterns.csv
```

Or simply:
```bash
make all
```

---

## üìä Expected Outputs

After running the full pipeline on 50 products:

### `product_info.csv` (~50 rows)
| asin | cohort | intervention_type | phi_any | k_any | l_max | sampling_weight |
|------|--------|------------------|---------|-------|-------|----------------|
| B08... | R | supplement | false | false | 2 | 0.97 |
| B09... | T | device | true | false | 5 | N/A |

### `claims.csv` (~100-200 rows)
| claim_id | claim_text_verbatim | gate_hint | source | confidence |
|----------|---------------------|-----------|--------|-----------|
| B08..._c0001 | "Clinically proven to improve sleep..." | l_soft | html | 0.90 |
| B09..._c0002 | "Quantum energy field healing..." | phi_candidate | html | 0.90 |
| B09..._c0003 | "Miracle cure" | l_hard | image | 0.70 |

### `assets_index.csv` (~150-500 rows)
| asin | asset_id | asset_type | url | sha256 |
|------|----------|-----------|-----|--------|
| B08... | B08..._img00 | image | https://... | abc123... |

---

## üéì Key Design Decisions

### Why Google Custom Search API?
- ‚úÖ **No Amazon API access required**: Official Product Advertising API requires approval
- ‚úÖ **Public data only**: Only indexes publicly accessible pages
- ‚úÖ **Reproducible**: Search results can be archived via Wayback Machine
- ‚úÖ **Ethical**: Respects robots.txt, transparent User-Agent
- ‚ö†Ô∏è **Limitation**: 100 queries/day free tier (upgrade to paid for scale)

### Why Gemini 2.0 Flash?
- ‚úÖ **Cost-effective**: ~$0.10 per 1M tokens (vs GPT-4: $30/1M)
- ‚úÖ **Fast**: Low latency for real-time extraction
- ‚úÖ **JSON mode**: Native structured output
- ‚úÖ **Free tier**: 1500 requests/day
- ‚úÖ **temp=0**: Fully deterministic

### Why Tesseract OCR?
- ‚úÖ **Open source**: No API costs
- ‚úÖ **Local processing**: No data sent to cloud
- ‚úÖ **Bounding boxes**: Preserves spatial information
- ‚úÖ **Confidence scores**: Enables quality filtering
- ‚ö†Ô∏è **Limitation**: Lower accuracy than cloud OCR (Google Vision, AWS Textract)

---

## üîç Reproducibility Guarantees

### 5-Minute Repro Card

1. **Input snapshots**: `product_urls.csv` (ASIN list)
2. **Page snapshots**: Wayback URLs in `pages.sha256.csv`
3. **HTML archives**: `data/raw/html/*.html` (SHA256 hashed)
4. **Extraction logs**: JSONL with model, temperature, timestamp
5. **Policy fingerprints**: All YAML configs versioned

### Verification
```bash
# Check extraction was deterministic
grep "temperature" data/extracted/claims_raw.jsonl
# Expected: All lines show "temperature": 0

# Check HTML integrity
sha256sum data/raw/html/B08XYZ123.html
# Compare to `page_sha256` in pages.sha256.csv
```

---

## üí∞ Cost Estimates (1000 Products)

| Service | Free Tier | Cost (Paid) | Time |
|---------|----------|------------|------|
| Google Custom Search | 100/day | $5 per 1000 | 10 days (free) or instant (paid) |
| Gemini API | 1500/day | ~$1-2 | 1-2 days (free) or instant (paid) |
| Tesseract OCR | Unlimited | $0 | N/A |
| Wayback Machine | Unlimited | $0 | N/A |
| **TOTAL** | **Free** | **~$6-7** | **10-12 days (free tier)** |

---

## ‚öñÔ∏è Ethical Compliance Checklist

- [x] **robots.txt respected**: `check_robots_txt()` uses urllib.robotparser
- [x] **Throttling enforced**: 3s default (configurable)
- [x] **Transparent User-Agent**: `TERVYX-Protocol-Research-Bot/1.0`
- [x] **Wayback archival**: All pages saved to Internet Archive
- [x] **SHA256 hashing**: Reproducible snapshots
- [x] **Rate limiting**: 100 requests/hour (configurable)
- [x] **Public pages only**: No login, no paywalls
- [x] **Scope restrictions**: Targeted categories/keywords only

---

## üêõ Known Limitations & Future Work

### Current Limitations

1. **Access constraints**:
   - Amazon may block automated access (use longer throttles, 5-10s)
   - robots.txt may restrict certain pages

2. **OCR quality**:
   - Tesseract accuracy lower than cloud OCR
   - Image-based claims may need manual review

3. **Ingredient parsing**:
   - Raw text extraction only
   - Normalization needs refinement (e.g., CAS numbers, IUPAC names)

4. **Population weights**:
   - Currently uses sampling plan allocation as proxy
   - Needs external benchmarks (Amazon category statistics)

### Roadmap

- [ ] **Integrate Amazon Product Advertising API** (if access granted)
- [ ] **Upgrade to cloud OCR** (Google Vision API)
- [ ] **Add FDA warning list auto-update** (scrape FDA website)
- [ ] **Implement claim deduplication** (cosine similarity)
- [ ] **Expand T cohort** with manual seed list (known problematic ASINs)
- [ ] **Multi-language support** (non-English claims)
- [ ] **Real-time monitoring** (track claims over time)

---

## üéØ Next Steps (50-Product Prototype)

### Phase 1: API Setup (30 min)
1. Create Google Custom Search Engine: https://programmablesearchengine.google.com/
2. Get Google Cloud API key: https://console.cloud.google.com/apis/credentials
3. Get Gemini API key: https://aistudio.google.com/app/apikey
4. Set environment variables (see SETUP.md)

### Phase 2: Install Dependencies (15 min)
```bash
pip install -r requirements.txt
brew install tesseract  # macOS
# or: sudo apt-get install tesseract-ocr  # Linux
```

### Phase 3: Run Prototype (1-2 hours)
```bash
# Generate 50 URLs (uses Google CSE)
python scripts/1_generate_urls.py --plan configs/sampling_plan.yaml --out data/raw/product_urls.csv --sample 50

# Scrape (3s throttle √ó 50 = 2.5 min + overhead)
python scripts/2_scrape_pages.py --in data/raw/product_urls.csv --out data/extracted/pages.sha256.csv --assets data/processed/assets_index.csv --html-dir data/raw/html/ --policy configs/scraping_policy.yaml

# Extract (Gemini API, ~30-60 min)
python scripts/3_extract_claims.py --pages data/extracted/pages.sha256.csv --assets data/processed/assets_index.csv --html-dir data/raw/html/ --out data/extracted/claims_raw.jsonl --policy configs/extraction_policy.yaml

# Normalize + validate + weights
make normalize validate weights report
```

### Phase 4: Verify Outputs (5 min)
```bash
# Check record counts
wc -l data/processed/product_info.csv  # Should be ~51 (50 + header)
wc -l data/processed/claims.csv        # Should be ~100-200

# Check temperature enforcement
grep "temperature" data/extracted/claims_raw.jsonl | head -1
# Expected: "temperature": 0

# Check sampling weights
head data/processed/product_info.csv
```

### Phase 5: Review Results
- Open `data/processed/claims.csv` in Excel/Sheets
- Look for Œ¶/K/L gate hints
- Check claim quality (verbatim text, no paraphrasing)
- Validate extraction confidence scores

---

## üìû Support

For issues, see:
- **Setup guide**: [SETUP.md](SETUP.md)
- **Main README**: [README.md](README.md)
- **GitHub issues**: [Issues](https://github.com/moneypuzzler/tervyx-amazon-claims/issues)

---

## ‚ú® Summary

**Mission Complete**: All core pipeline components have been implemented using the **Option B (Google Custom Search API)** approach. The system is ready for:

1. ‚úÖ **50-product prototype** (1-2 hours, requires API keys)
2. ‚úÖ **1000-product full run** (10-12 days free tier, or instant with paid APIs)
3. ‚úÖ **Nature-grade reproducibility** (SHA256, Wayback, temp=0)

**Key Achievement**: Transformed placeholder implementations into a production-ready pipeline with:
- Google Custom Search API integration
- Gemini 2.0 Flash extraction (temp=0)
- Tesseract OCR pipeline
- Ethical scraping (robots.txt, throttling, Wayback)
- Stratified sampling weights

**Cost**: ~$6-7 for 1000 products (or free with 10-12 day collection period)

**Next**: Run 50-product prototype to validate pipeline end-to-end! üöÄ
